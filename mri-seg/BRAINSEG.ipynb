{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe3b333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nibabel in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (5.3.3)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from nibabel) (6.5.2)\n",
      "Requirement already satisfied: numpy>=1.22 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from nibabel) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from nibabel) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from nibabel) (4.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/ef/69/de33bd90dbddc8eede8f99ddeccfb374f7e18f84beb404bfe2cbbdf8df90/tensorflow-2.20.0-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading tensorflow-2.20.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/8f/aa/ba0014cc4659328dc818a28827be78e6d97312ab0cb98105a770924dc11e/absl_py-2.3.1-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=24.3.25 from https://files.pythonhosted.org/packages/e8/2d/d2a548598be01649e2d46231d151a6c56d10b964d94043a335ae56ea2d92/flatbuffers-25.12.19-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 from https://files.pythonhosted.org/packages/1d/33/f1c6a276de27b7d7339a34749cc33fa87f077f921969c47185d34a887ae2/gast-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Obtaining dependency information for google_pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/4b/49/f5e3e7e1419872b69f6f5e82ba56e33955a74bd537d8a1f5f1eff2f3668a/libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Obtaining dependency information for opt_einsum>=2.3.2 from https://files.pythonhosted.org/packages/23/cd/066e86230ae37ed0be70aae89aabf03ca8d9f39c8aea0dec8029455b5540/opt_einsum-3.4.0-py3-none-any.whl.metadata\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Obtaining dependency information for protobuf>=5.28.0 from https://files.pythonhosted.org/packages/66/15/6ee23553b6bfd82670207ead921f4d8ef14c107e5e11443b04caeb5ab5ec/protobuf-6.33.4-cp39-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-6.33.4-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/33/d1/8bb87d21e9aeb323cc03034f5eaf2c8f69841e40e4853c2627edf8111ed3/termcolor-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/10/c1/934202f5cf335e6d852530ce14ddb0fef21be612ba9ecbbcbd4d748ca32d/grpcio-1.76.0-cp311-cp311-macosx_11_0_universal2.whl.metadata\n",
      "  Downloading grpcio-1.76.0-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorboard~=2.20.0 from https://files.pythonhosted.org/packages/9c/d9/a5db55f88f258ac669a92858b70a714bbbd5acd993820b41ec4a96a4d77f/tensorboard-2.20.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Obtaining dependency information for keras>=3.10.0 from https://files.pythonhosted.org/packages/d8/e5/8b40bada1f33f25deca7bad0e8c7ca6752f2b09e8018e2fc4693858dd662/keras-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading keras-3.13.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy>=1.26.0 (from tensorflow)\n",
      "  Obtaining dependency information for numpy>=1.26.0 from https://files.pythonhosted.org/packages/8e/ba/80fc0b1e3cb2fd5c6143f00f42eb67762aa043eaa05ca924ecc3222a7849/numpy-2.4.1-cp311-cp311-macosx_14_0_arm64.whl.metadata\n",
      "  Downloading numpy-2.4.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Obtaining dependency information for h5py>=3.11.0 from https://files.pythonhosted.org/packages/c1/b0/1c628e26a0b95858f54aba17e1599e7f6cd241727596cc2580b72cb0a9bf/h5py-3.15.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading h5py-3.15.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Obtaining dependency information for ml_dtypes<1.0.0,>=0.5.1 from https://files.pythonhosted.org/packages/c6/5e/712092cfe7e5eb667b8ad9ca7c54442f21ed7ca8979745f1000e24cf8737/ml_dtypes-0.5.4-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading ml_dtypes-0.5.4-cp311-cp311-macosx_10_9_universal2.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting typing_extensions>=3.6.6 (from tensorflow)\n",
      "  Obtaining dependency information for typing_extensions>=3.6.6 from https://files.pythonhosted.org/packages/18/67/36e9267722cc04a6b9f15c7f3441c2363321a3ea07da7ae0c0707beb2a9c/typing_extensions-4.15.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Obtaining dependency information for rich from https://files.pythonhosted.org/packages/25/7a/b0178788f8dc6cafce37a212c99565fa1fe7872c70c6c9c1e1a372d9d88f/rich-14.2.0-py3-none-any.whl.metadata\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Obtaining dependency information for namex from https://files.pythonhosted.org/packages/b2/bc/465daf1de06409cdd4532082806770ee0d8d7df434da79c76564d0f69741/namex-0.1.0-py3-none-any.whl.metadata\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Obtaining dependency information for optree from https://files.pythonhosted.org/packages/d4/f6/5377f265a8dcd61edabf8b87b657d78fca9051eeaf311ed77f73b43526a9/optree-0.18.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading optree-0.18.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pillow in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow) (9.4.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/chloechristensen/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.20.0-cp311-cp311-macosx_12_0_arm64.whl (200.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.76.0-cp311-cp311-macosx_11_0_universal2.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.15.1-cp311-cp311-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.13.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp311-cp311-macosx_10_9_universal2.whl (679 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.7/679.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.1-cp311-cp311-macosx_14_0_arm64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.4-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.6/427.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp311-cp311-macosx_11_0_arm64.whl (337 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, typing_extensions, termcolor, tensorboard-data-server, protobuf, opt_einsum, numpy, google_pasta, gast, astunparse, absl-py, rich, optree, ml_dtypes, h5py, grpcio, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 2.4.1 which is incompatible.\n",
      "scipy 1.11.1 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.12.19 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 keras-3.13.1 libclang-18.1.1 ml_dtypes-0.5.4 namex-0.1.0 numpy-2.4.1 opt_einsum-3.4.0 optree-0.18.0 protobuf-6.33.4 rich-14.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0 typing_extensions-4.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nibabel\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    import kagglehub  # type: ignore\n",
    "except ModuleNotFoundError:\n",
    "    kagglehub = None\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Dataset location\n",
    "if kagglehub is not None:\n",
    "    path = kagglehub.dataset_download(\"shakilrana/brats-2023-adult-glioma\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "else:\n",
    "    dataset_path = \"/kaggle/input/brats-2023-adult-glioma\"\n",
    "    print(\"kagglehub not installed; set dataset_path manually:\", dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Define dataset paths\n",
    "# For example, dataset_path is the root download folder on Kaggle:\n",
    "dataset_path = \"/kaggle/input/brats-2023-adult-glioma\"\n",
    "\n",
    "# Define the two primary base paths\n",
    "training_path = os.path.join(dataset_path, \"ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData\")\n",
    "validation_path = os.path.join(\n",
    "    dataset_path,\n",
    "    \"ASNR-MICCAI-BraTS2023-GLI-Challenge-ValidationData\",\n",
    "    \"ASNR-MICCAI-BraTS2023-GLI-Challenge-ValidationData\",\n",
    ")\n",
    "\n",
    "# List subject directories (folders like 'BraTS-GLI-xxxxx-xxx') in each folder\n",
    "train_dirs = sorted(glob.glob(os.path.join(training_path, \"BraTS-GLI-*\")))\n",
    "val_dirs = sorted(glob.glob(os.path.join(validation_path, \"BraTS-GLI-*\")))\n",
    "\n",
    "# Combine both lists into one pool of cases\n",
    "all_cases = train_dirs + val_dirs\n",
    "print(\"Total cases found:\", len(all_cases))\n",
    "\n",
    "# Split cases into 70% training, 15% validation, and 15% testing\n",
    "train_cases, temp_cases = train_test_split(all_cases, test_size=0.3, random_state=SEED)\n",
    "val_cases, test_cases = train_test_split(temp_cases, test_size=0.5, random_state=SEED)\n",
    "\n",
    "print(\"Number of training cases:\", len(train_cases))\n",
    "print(\"Number of validation cases:\", len(val_cases))\n",
    "print(\"Number of test cases:\", len(test_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471943fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Setup: Random Seeds and Dataset Paths\n",
    "# -------------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# We will work only with the training folder, as it contains segmentation masks.\n",
    "dataset_path = \"/kaggle/input/brats-2023-adult-glioma\"\n",
    "training_path = os.path.join(dataset_path, \"ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. List All Case Directories Recursively\n",
    "# -------------------------------\n",
    "# Some case folders may be nested. Use a recursive glob search.\n",
    "train_dirs = sorted(glob.glob(os.path.join(training_path, \"**\", \"BraTS-GLI-*\"), recursive=True))\n",
    "print(\"Total training cases found:\", len(train_dirs))\n",
    "\n",
    "if len(train_dirs) == 0:\n",
    "    raise ValueError(\"No training cases were found. Check your dataset path and glob pattern.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Split Cases into Training and Testing (Use 'train' for training and 'val' for testing)\n",
    "# -------------------------------\n",
    "# Here we take 70% of the cases for training and 30% for testing.\n",
    "train_cases, val_cases = train_test_split(train_dirs, test_size=0.3, random_state=SEED)\n",
    "print(\"Number of training cases:\", len(train_cases))\n",
    "print(\"Number of test cases (to be used as val):\", len(val_cases))\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Helper Functions for Loading and Finding Files\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "def load_volume(file_path):\n",
    "    \"\"\"Loads a NIfTI file and returns its 3D data array.\"\"\"\n",
    "    volume = nib.load(file_path).get_fdata()\n",
    "    return volume\n",
    "\n",
    "\n",
    "def get_modality_file(case_dir, modality=\"flair\"):\n",
    "    \"\"\"\n",
    "    Searches recursively for a NIfTI file in the case directory that contains the modality keyword.\n",
    "    Expected modality keywords: 'flair', 't1', 't1c', 't2', or 'seg'\n",
    "    \"\"\"\n",
    "    nii_files = glob.glob(os.path.join(case_dir, \"**\", \"*.nii*\"), recursive=True)\n",
    "    for file in nii_files:\n",
    "        if modality.lower() in file.lower():\n",
    "            return file\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Debug: Check a Sample Case for Segmentation File\n",
    "# -------------------------------\n",
    "sample_case = train_cases[0]\n",
    "seg_file_sample = get_modality_file(sample_case, modality=\"seg\")\n",
    "print(\"Sample case folder:\", sample_case)\n",
    "print(\"Contents of sample case:\", os.listdir(sample_case))\n",
    "print(\"Segmentation file found for sample case:\", seg_file_sample)\n",
    "# At this point, you should see a valid segmentation file path.\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Generator to Yield a Random 2D Slice (Resized to 128x128)\n",
    "# -------------------------------\n",
    "def generate_examples(case_list, modality=\"flair\"):\n",
    "    \"\"\"\n",
    "    For each case in case_list:\n",
    "      - Loads the specified modality (e.g., 'flair') and its segmentation ('seg').\n",
    "      - Selects a random slice from the 3D volumes.\n",
    "      - Normalizes the image slice to [0, 1] and resizes both image and segmentation to (128, 128).\n",
    "      - Yields a tuple (image_slice, seg_slice).\n",
    "    \"\"\"\n",
    "    for case in case_list:\n",
    "        image_file = get_modality_file(case, modality=modality)\n",
    "        seg_file = get_modality_file(case, modality=\"seg\")\n",
    "\n",
    "        if image_file is None:\n",
    "            print(f\"Image file not found for case: {case}\")\n",
    "            continue\n",
    "        if seg_file is None:\n",
    "            print(f\"Segmentation file not found for case: {case}\")\n",
    "            continue\n",
    "\n",
    "        # Load volumes (assumed to be 3D arrays)\n",
    "        image_vol = load_volume(image_file)\n",
    "        seg_vol = load_volume(seg_file)\n",
    "\n",
    "        # Check if volumes are valid\n",
    "        if image_vol.shape[2] <= 0 or seg_vol.shape[2] <= 0:\n",
    "            continue\n",
    "\n",
    "        # Randomly select a slice index\n",
    "        slice_idx = random.randint(0, image_vol.shape[2] - 1)\n",
    "        image_slice = image_vol[:, :, slice_idx]\n",
    "        seg_slice = seg_vol[:, :, slice_idx]\n",
    "\n",
    "        # Normalize image slice to [0,1]\n",
    "        image_slice = image_slice.astype(np.float32)\n",
    "        if image_slice.max() > 0:\n",
    "            image_slice /= image_slice.max()\n",
    "\n",
    "        # Expand dims to have channel dimension (H, W, 1)\n",
    "        image_slice = np.expand_dims(image_slice, axis=-1)\n",
    "\n",
    "        # Resize both image and segmentation to (128, 128)\n",
    "        image_slice = tf.image.resize(image_slice, (128, 128)).numpy()\n",
    "        seg_slice = tf.image.resize(np.expand_dims(seg_slice, axis=-1), (128, 128), method=\"nearest\").numpy()\n",
    "        seg_slice = np.squeeze(seg_slice, axis=-1).astype(np.int32)\n",
    "\n",
    "        yield image_slice, seg_slice\n",
    "\n",
    "        # -------------------------------\n",
    "\n",
    "\n",
    "# 7. Create tf.data.Dataset Objects for Training and Testing\n",
    "# -------------------------------\n",
    "BATCH_SIZE = 4  # Adjust as needed based on memory and TPU usage\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_examples(train_cases, modality=\"flair\"),\n",
    "    output_types=(tf.float32, tf.int32),\n",
    "    output_shapes=((128, 128, 1), (128, 128)),\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(buffer_size=20).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_examples(val_cases, modality=\"flair\"),\n",
    "    output_types=(tf.float32, tf.int32),\n",
    "    output_shapes=((128, 128, 1), (128, 128)),\n",
    ")\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Debug: Check That the Generator Yields Batches\n",
    "# -------------------------------\n",
    "batch_count = 0\n",
    "for _ in train_dataset.take(1):\n",
    "    batch_count += 1\n",
    "print(\"Number of batches from training generator (should be > 0):\", batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df90d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Helper functions for visualization\n",
    "# -------------------------------\n",
    "\n",
    "# Assumes `load_volume` and `get_modality_file` are defined in an earlier cell.\n",
    "\n",
    "\n",
    "def plot_middle_slice(volume, ax, title=\"\"):\n",
    "    \"\"\"\n",
    "    Plots the middle slice (along the third axis) of a 3D volume on the given matplotlib axes.\n",
    "    \"\"\"\n",
    "    slice_index = volume.shape[2] // 2  # choose the middle slice\n",
    "    ax.imshow(volume[:, :, slice_index], cmap=\"gray\")\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def visualize_samples(case_list, n_samples=2, modality=\"flair\", set_name=\"\"):\n",
    "    \"\"\"\n",
    "    Randomly selects n_samples cases from case_list, loads the modality volume,\n",
    "    and visualizes the middle slice for each.\n",
    "    \"\"\"\n",
    "    samples = random.sample(case_list, n_samples)\n",
    "    fig, axes = plt.subplots(1, n_samples, figsize=(15, 5))\n",
    "\n",
    "    # If only one sample is selected, adjust axes to be iterable.\n",
    "    if n_samples == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, case in enumerate(samples):\n",
    "        mod_file = get_modality_file(case, modality=modality)\n",
    "        if mod_file is not None:\n",
    "            volume = load_volume(mod_file)\n",
    "            case_name = os.path.basename(case)\n",
    "            title = f\"{set_name}\\nCase: {case_name}\"\n",
    "            plot_middle_slice(volume, ax=axes[i], title=title)\n",
    "        else:\n",
    "            axes[i].text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                \"Modality file not found\",\n",
    "                horizontalalignment=\"center\",\n",
    "                verticalalignment=\"center\",\n",
    "            )\n",
    "            axes[i].set_title(f\"{set_name}\\nCase: {os.path.basename(case)}\")\n",
    "            axes[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Optional: Debug Step - Inspect one case folder\n",
    "# -------------------------------\n",
    "sample_case = train_cases[0]\n",
    "print(\"Sample case folder:\", sample_case)\n",
    "print(\"Contents of sample case:\", os.listdir(sample_case))\n",
    "\n",
    "# -------------------------------\n",
    "# Visualize 2 random samples from each split using the 'flair' modality\n",
    "# -------------------------------\n",
    "visualize_samples(train_cases, n_samples=2, modality=\"flair\", set_name=\"Training\")\n",
    "visualize_samples(val_cases, n_samples=2, modality=\"flair\", set_name=\"Validation\")\n",
    "visualize_samples(test_cases, n_samples=2, modality=\"flair\", set_name=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TPU Setup (if using TPU)\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Detect TPU\n",
    "    print(\"Running on TPU:\", tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print(\"TPU not found. Using CPU/GPU strategy.\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbbeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions and datasets are defined in earlier cells.\n",
    "# `train_dataset` and `val_dataset` are available for training below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf20681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    def conv_block(x, filters):\n",
    "        x = tf.keras.layers.Conv2D(filters, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "        x = tf.keras.layers.Conv2D(filters, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "        return x\n",
    "\n",
    "    def build_unet(input_shape=(128, 128, 1), num_classes=4):\n",
    "        inputs = tf.keras.Input(input_shape)\n",
    "\n",
    "        # Encoder path\n",
    "        c1 = conv_block(inputs, 64)\n",
    "        p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "        c2 = conv_block(p1, 128)\n",
    "        p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "        c3 = conv_block(p2, 256)\n",
    "        p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "        c4 = conv_block(p3, 512)\n",
    "        p4 = tf.keras.layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "        # Bottleneck\n",
    "        c5 = conv_block(p4, 1024)\n",
    "\n",
    "        # Decoder path\n",
    "        u6 = tf.keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding=\"same\")(c5)\n",
    "        u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "        c6 = conv_block(u6, 512)\n",
    "\n",
    "        u7 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding=\"same\")(c6)\n",
    "        u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "        c7 = conv_block(u7, 256)\n",
    "\n",
    "        u8 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding=\"same\")(c7)\n",
    "        u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "        c8 = conv_block(u8, 128)\n",
    "\n",
    "        u9 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=\"same\")(c8)\n",
    "        u9 = tf.keras.layers.concatenate([u9, c1])\n",
    "        c9 = conv_block(u9, 64)\n",
    "\n",
    "        outputs = tf.keras.layers.Conv2D(num_classes, (1, 1), activation=\"softmax\")(c9)\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        return model\n",
    "\n",
    "    # Assume 4 segmentation classes: background + 3 tumor sub-regions\n",
    "    model = build_unet(input_shape=(128, 128, 1), num_classes=4)\n",
    "\n",
    "    # Compile the model with sparse categorical crossentropy (since segmentation masks are integer labels)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e8f6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
